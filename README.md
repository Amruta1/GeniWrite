# GeniWrite
Implementation of a GPT-like model from scratch using multi-head attention in PyTorch (educational & experimental). Includes training, inference, and text generation examples.

Features

✅ Multi-Head Self Attention implemented from scratch

✅ Transformer block (Attention + Feedforward + LayerNorm + Residuals)

✅ Positional Embeddings for sequence order

✅ Causal Masking for autoregressive text generation

✅ Training loop for next-token prediction

✅ Text generation with temperature & greedy decoding
